{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/miniconda3/envs/bert24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/coder/miniconda3/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:958: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/coder/miniconda3/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:1017: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.bert_layers.softpick import naive_softpick_attn, parallel_softpick_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 2])\n",
      "tensor([0, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "def pack_sequences(inputs, attention_mask):\n",
    "    \"\"\"\n",
    "    inputs: Tensor of shape [batch_size, max_seq_len, ...]\n",
    "    attention_mask: Tensor of shape [batch_size, max_seq_len] (1=valid, 0=pad)\n",
    "    \"\"\"\n",
    "    # Get actual sequence lengths\n",
    "    seq_lens = attention_mask.sum(dim=1).cpu().tolist()\n",
    "    \n",
    "    # Remove padding and concatenate sequences\n",
    "    packed = torch.cat([inputs[i,:l] for i,l in enumerate(seq_lens)], dim=0)\n",
    "    \n",
    "    # Create cumulative sequence lengths\n",
    "    cu_seqlens = torch.tensor(\n",
    "        [0] + [sum(seq_lens[:i+1]) for i in range(len(seq_lens))],\n",
    "        dtype=torch.long,\n",
    "        device=inputs.device\n",
    "    )\n",
    "    \n",
    "    return packed.unsqueeze(0), cu_seqlens  # Add batch dimension\n",
    "\n",
    "# Original padded batch\n",
    "batch = torch.tensor([\n",
    "    [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.0, 0.0]],  # seq_len=3\n",
    "    [[0.7, 0.8], [0.9, 1.0], [0.0, 0.0], [0.0, 0.0]]   # seq_len=2\n",
    "])\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0],\n",
    "    [1, 1, 0, 0]\n",
    "])\n",
    "\n",
    "# Pack sequences\n",
    "packed, cu_seqlens = pack_sequences(batch, mask)\n",
    "print(packed.shape)  # torch.Size([1, 5, 2]) \n",
    "print(cu_seqlens)    # tensor([0, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing naive softpick attention...\n",
      "Testing parallel softpick attention...\n",
      "tensor([[[[-1.7523e+00,  4.1025e-01, -8.9073e-01,  ..., -1.3521e+00,\n",
      "            3.6076e-01,  5.1788e-01],\n",
      "          [ 2.8459e-01, -1.1112e+00, -9.3327e-01,  ..., -7.5988e-02,\n",
      "           -7.6570e-01,  1.4538e+00],\n",
      "          [ 6.3318e-01, -6.1629e-01, -2.4211e+00,  ...,  5.6732e-01,\n",
      "           -2.0947e+00, -5.4335e-01],\n",
      "          ...,\n",
      "          [-7.7210e-01,  8.7004e-01, -4.4575e-01,  ..., -1.3027e+00,\n",
      "            1.2381e+00, -7.3564e-01],\n",
      "          [-1.4301e+00,  3.9622e-01, -2.1280e+00,  ...,  1.4532e-01,\n",
      "            6.7443e-01,  1.5650e+00],\n",
      "          [-7.6796e-01, -1.0822e+00, -1.8001e+00,  ..., -1.1054e+00,\n",
      "           -7.2440e-01,  5.2213e-01]],\n",
      "\n",
      "         [[ 4.5712e-01,  6.0572e-01, -1.3262e-01,  ..., -1.7929e+00,\n",
      "            2.5354e-01, -6.5786e-01],\n",
      "          [ 1.0021e+00,  3.2965e-01,  7.7627e-01,  ...,  1.7770e+00,\n",
      "           -3.5784e-01,  1.5578e+00],\n",
      "          [-1.3292e+00,  1.0739e+00, -1.2882e-01,  ..., -7.7871e-01,\n",
      "           -9.5401e-01,  3.5194e-01],\n",
      "          ...,\n",
      "          [ 6.3591e-01,  1.7850e+00,  6.3318e-01,  ..., -6.0338e-01,\n",
      "            8.4192e-01, -8.9898e-01],\n",
      "          [-1.4028e+00, -3.7334e-01, -4.8789e-01,  ..., -1.1343e-01,\n",
      "            5.7039e-02,  1.4486e+00],\n",
      "          [ 9.3802e-01, -3.1077e-01,  1.3629e+00,  ...,  2.3032e-01,\n",
      "            4.3373e-01, -9.2715e-01]],\n",
      "\n",
      "         [[-1.7325e+00,  1.5277e+00, -1.3814e-01,  ..., -5.9642e-01,\n",
      "            8.6081e-01,  1.8442e+00],\n",
      "          [ 1.3440e+00, -1.0028e+00, -8.3888e-01,  ...,  3.7073e-01,\n",
      "           -7.2270e-01,  9.0465e-01],\n",
      "          [ 1.0461e+00,  2.4951e-01, -1.0167e+00,  ..., -1.2503e+00,\n",
      "            1.2833e+00,  8.8496e-01],\n",
      "          ...,\n",
      "          [ 4.2723e-01, -1.5688e+00, -8.7290e-01,  ..., -4.7547e-01,\n",
      "            1.8405e-01,  9.1145e-01],\n",
      "          [ 5.9841e-01, -1.5274e+00,  8.3630e-01,  ..., -5.6501e-01,\n",
      "           -3.3539e-01,  2.2936e+00],\n",
      "          [ 5.9841e-01, -1.5274e+00,  8.3630e-01,  ..., -5.6501e-01,\n",
      "           -3.3539e-01,  2.2936e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8072e+00,  6.8167e-01, -4.5368e-01,  ..., -1.3650e+00,\n",
      "            1.4793e+00, -1.8651e+00],\n",
      "          [-2.1368e-01, -1.0370e+00, -9.2614e-01,  ...,  1.0754e-01,\n",
      "            1.5443e+00, -1.1685e-01],\n",
      "          [ 1.7455e+00,  2.5719e-03,  7.2035e-01,  ...,  4.0889e-01,\n",
      "            2.0670e-01,  4.8489e-01],\n",
      "          ...,\n",
      "          [-1.8465e-01, -1.7116e+00,  1.9936e+00,  ...,  2.6681e-01,\n",
      "            1.3604e+00, -2.3374e-01],\n",
      "          [ 6.8753e-01,  1.9625e-01, -6.8967e-01,  ..., -3.1284e-01,\n",
      "           -3.3078e-02, -6.5515e-01],\n",
      "          [-1.3966e+00, -5.3417e-02,  9.4117e-01,  ...,  9.0263e-01,\n",
      "           -9.5419e-02, -1.1551e+00]],\n",
      "\n",
      "         [[-1.1408e+00,  3.2251e-01, -5.9076e-01,  ..., -2.8113e-01,\n",
      "            8.3835e-01, -4.4444e-01],\n",
      "          [-4.2391e-01, -5.8421e-01, -4.5920e-01,  ...,  5.4877e-02,\n",
      "           -1.1989e+00,  1.0264e+00],\n",
      "          [ 6.8101e-01, -8.5914e-01, -1.2832e+00,  ...,  2.1892e-01,\n",
      "           -1.1042e-01,  1.0517e+00],\n",
      "          ...,\n",
      "          [ 1.0058e-01,  1.1156e-01, -3.7334e-01,  ..., -3.8086e-01,\n",
      "            4.2352e-01, -4.2686e-01],\n",
      "          [ 1.2902e+00, -2.7291e-01, -5.6596e-01,  ..., -1.0679e+00,\n",
      "           -6.3855e-01,  8.5473e-01],\n",
      "          [ 6.8095e-01, -9.8732e-01,  6.1413e-01,  ...,  2.6612e+00,\n",
      "           -2.9360e-01, -7.6164e-01]],\n",
      "\n",
      "         [[-7.3837e-01, -9.8388e-03, -8.6403e-01,  ..., -1.3593e-01,\n",
      "           -1.3682e+00, -1.0684e+00],\n",
      "          [ 9.3173e-01, -8.9052e-01,  2.5274e-02,  ...,  1.7314e+00,\n",
      "            1.4633e-03,  4.0744e-01],\n",
      "          [ 4.5943e-01, -1.8600e+00, -2.1558e+00,  ..., -4.7945e-01,\n",
      "           -5.6434e-01, -3.8163e-01],\n",
      "          ...,\n",
      "          [ 8.4653e-03, -1.2623e+00, -2.6457e-01,  ...,  1.8718e-01,\n",
      "            3.3289e-01,  1.7962e-02],\n",
      "          [-3.5235e-01,  7.8121e-01,  7.6105e-01,  ..., -1.4833e+00,\n",
      "           -1.5790e+00,  1.9780e+00],\n",
      "          [-8.0531e-01, -1.0725e+00,  3.4849e-02,  ..., -1.3778e-01,\n",
      "           -1.6171e-01, -7.3238e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.5596e-01, -3.5518e-01, -4.7836e-01,  ...,  4.8964e-01,\n",
      "            1.0708e+00,  7.0552e-01],\n",
      "          [-7.0787e-01, -4.6168e-01,  5.9763e-01,  ...,  8.5500e-02,\n",
      "            1.2544e+00, -1.3925e+00],\n",
      "          [ 1.2948e+00,  1.7356e+00,  4.3288e-01,  ...,  8.4386e-01,\n",
      "           -1.5208e-03, -7.0721e-01],\n",
      "          ...,\n",
      "          [-8.3699e-01, -2.1553e-01,  2.1909e+00,  ..., -1.0721e+00,\n",
      "           -2.2099e-01,  6.0013e-01],\n",
      "          [ 4.8443e-01, -1.6732e-01, -2.2479e-03,  ..., -1.6728e-01,\n",
      "            1.1184e+00,  1.0572e+00],\n",
      "          [ 5.6868e-01, -1.4953e+00, -5.1197e-01,  ...,  5.0844e-01,\n",
      "            3.2770e-01, -3.5805e-02]],\n",
      "\n",
      "         [[-9.5329e-01,  7.0912e-01,  1.2514e-01,  ..., -3.1789e-01,\n",
      "           -1.3577e+00, -5.2413e-01],\n",
      "          [-3.4936e+00, -2.6139e-01,  8.3696e-01,  ...,  1.2906e+00,\n",
      "           -1.3319e+00, -8.3266e-01],\n",
      "          [ 1.1739e+00,  2.2987e+00, -2.0170e-01,  ..., -1.1397e-01,\n",
      "            7.5171e-02,  6.7203e-01],\n",
      "          ...,\n",
      "          [-1.1007e+00,  1.3984e+00, -2.7433e+00,  ...,  3.8606e-01,\n",
      "            1.8249e+00,  1.2336e+00],\n",
      "          [-6.2117e-01,  6.2011e-01, -4.7667e-01,  ...,  5.5836e-01,\n",
      "           -3.7915e-01,  1.4478e+00],\n",
      "          [ 2.9074e-01, -9.0627e-01, -1.9710e+00,  ...,  1.7994e+00,\n",
      "           -1.7400e+00,  6.1636e-01]],\n",
      "\n",
      "         [[ 1.8371e+00,  4.7224e-01, -3.9133e-01,  ..., -1.9417e-01,\n",
      "           -4.1879e-01,  7.5349e-01],\n",
      "          [ 2.2315e-01, -1.7585e-01, -3.7158e-01,  ...,  2.3747e-01,\n",
      "           -4.3493e-01, -3.7939e-01],\n",
      "          [ 5.7880e-01,  6.7126e-01, -3.3097e-01,  ...,  6.2221e-01,\n",
      "           -1.1894e+00,  1.3327e+00],\n",
      "          ...,\n",
      "          [ 4.7836e-01,  5.1894e-01,  9.2830e-01,  ...,  6.2157e-01,\n",
      "           -7.4434e-01,  6.7004e-01],\n",
      "          [ 1.8455e+00,  4.7038e-01, -3.8694e-01,  ..., -1.9385e-01,\n",
      "           -4.1862e-01,  7.5533e-01],\n",
      "          [ 1.7426e+00,  4.3395e-01,  4.6029e-01,  ..., -1.0634e+00,\n",
      "            2.3331e-01, -3.0885e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0759e-01,  1.8099e-01, -2.0256e-01,  ...,  2.0677e-01,\n",
      "           -6.8341e-01, -6.3177e-02],\n",
      "          [-2.2066e-01,  2.0084e-02,  4.9870e-01,  ...,  1.2870e-01,\n",
      "            8.6954e-01,  1.2201e-01],\n",
      "          [ 1.2239e+00, -1.6940e+00, -6.8465e-01,  ...,  1.2171e+00,\n",
      "           -1.3974e+00,  8.8044e-01],\n",
      "          ...,\n",
      "          [ 2.6345e-01, -4.7749e-01, -5.7037e-01,  ...,  7.4131e-01,\n",
      "            1.8728e+00, -7.2228e-01],\n",
      "          [ 1.7464e+00,  1.9126e-01,  1.9998e+00,  ..., -7.7493e-01,\n",
      "            1.6341e+00, -2.8316e-01],\n",
      "          [-3.3388e-01,  2.1538e-01, -2.0630e-01,  ..., -1.8935e+00,\n",
      "           -3.4070e-01, -2.4557e-01]],\n",
      "\n",
      "         [[ 7.2024e-01, -1.3039e+00, -1.8950e+00,  ...,  3.4827e-01,\n",
      "            6.4335e-01,  2.2413e-01],\n",
      "          [-1.2054e+00, -1.2790e+00, -1.3639e+00,  ..., -1.0968e+00,\n",
      "            2.0726e+00, -3.5940e-01],\n",
      "          [-8.1119e-01, -8.3387e-02,  6.3037e-01,  ...,  7.4425e-01,\n",
      "            7.7137e-01,  1.1523e+00],\n",
      "          ...,\n",
      "          [-6.0882e-01, -2.4420e-01, -6.8405e-01,  ..., -2.5634e-01,\n",
      "            8.5451e-01,  5.9413e-01],\n",
      "          [ 1.1983e+00,  1.5440e+00,  1.5839e-01,  ...,  1.2570e-01,\n",
      "           -5.2498e-01,  1.5363e-01],\n",
      "          [ 1.2653e+00,  6.9000e-01,  1.4299e+00,  ...,  5.9766e-01,\n",
      "           -6.7647e-01, -4.1367e-01]],\n",
      "\n",
      "         [[ 1.6544e+00,  1.5930e-01, -2.9224e-01,  ...,  6.3709e-01,\n",
      "            7.6091e-01, -6.7182e-01],\n",
      "          [ 5.8852e-01, -1.2816e+00, -6.4234e-01,  ...,  3.6196e-01,\n",
      "           -2.4542e+00,  1.2503e-01],\n",
      "          [ 4.9585e-01,  7.6464e-01, -4.3998e-02,  ...,  4.7034e-01,\n",
      "            6.5225e-01,  7.8330e-01],\n",
      "          ...,\n",
      "          [ 2.2688e+00,  9.3374e-01, -1.6475e+00,  ..., -1.3343e+00,\n",
      "            1.4666e+00, -3.0525e-01],\n",
      "          [-5.5656e-01,  3.1879e-01, -5.1736e-01,  ..., -8.5257e-01,\n",
      "            1.8300e+00, -1.6431e+00],\n",
      "          [-3.2194e-01, -5.2751e-01, -5.1304e-01,  ...,  9.9283e-01,\n",
      "           -1.5314e+00, -5.5673e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0048e-01,  2.8138e+00,  8.4508e-01,  ..., -1.0960e-01,\n",
      "           -1.3125e+00, -1.4065e+00],\n",
      "          [ 1.9306e-01,  9.6729e-01,  2.0942e-01,  ...,  6.0078e-01,\n",
      "            2.2627e+00, -9.0220e-01],\n",
      "          [ 4.4879e-01,  7.1445e-01, -1.6980e+00,  ..., -8.5673e-01,\n",
      "           -6.0495e-01, -9.0645e-01],\n",
      "          ...,\n",
      "          [-2.1192e+00,  2.5413e-01,  7.6771e-01,  ...,  2.7758e-01,\n",
      "            8.5297e-02,  4.4186e-01],\n",
      "          [-1.0037e-01, -6.7395e-01,  1.3370e+00,  ...,  3.7385e-01,\n",
      "           -3.0640e-01, -8.4305e-01],\n",
      "          [ 6.7698e-01, -1.7708e+00, -1.4484e+00,  ..., -6.9428e-02,\n",
      "           -1.4995e-01, -1.2322e+00]],\n",
      "\n",
      "         [[-7.6130e-01,  1.1126e+00,  2.5439e+00,  ...,  8.7212e-02,\n",
      "            1.2703e+00,  9.4029e-01],\n",
      "          [-8.7759e-04,  5.1177e-01,  1.6129e-01,  ...,  1.2965e+00,\n",
      "           -1.0112e+00, -5.1351e-01],\n",
      "          [-2.5240e+00,  8.8209e-01, -1.4387e+00,  ...,  1.2569e-01,\n",
      "            1.4782e-01,  6.5520e-02],\n",
      "          ...,\n",
      "          [-1.3246e-01,  1.7979e+00, -6.7877e-01,  ..., -6.0993e-01,\n",
      "            3.3834e-01,  6.3800e-02],\n",
      "          [ 6.2256e-01,  3.4304e-01,  4.3499e-01,  ...,  1.3703e+00,\n",
      "            4.2979e-01, -5.4627e-01],\n",
      "          [ 3.5104e-01,  1.9325e+00,  1.6672e+00,  ...,  9.3485e-01,\n",
      "            1.3847e-01,  3.7289e-02]],\n",
      "\n",
      "         [[ 1.8341e-01, -5.5739e-01, -1.0249e-01,  ..., -1.2246e-01,\n",
      "           -5.4688e-01, -2.5819e+00],\n",
      "          [ 1.3306e+00,  7.1078e-01,  1.8864e-01,  ...,  1.4364e+00,\n",
      "           -2.3519e-01, -1.1682e+00],\n",
      "          [-4.1192e-01, -1.2587e+00, -1.9177e+00,  ...,  1.8179e+00,\n",
      "           -1.6252e+00,  2.3748e+00],\n",
      "          ...,\n",
      "          [ 1.3259e+00, -4.6226e-01, -3.6085e-01,  ..., -1.9029e-02,\n",
      "            6.5628e-01, -7.5700e-01],\n",
      "          [-9.7141e-02,  4.4955e-01, -7.0719e-01,  ..., -5.1363e-01,\n",
      "            9.3476e-01, -3.6941e-01],\n",
      "          [ 8.3106e-01,  6.0028e-01,  2.3337e+00,  ..., -1.7044e+00,\n",
      "            1.4036e+00,  1.5245e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.6060e-01, -5.2493e-01,  1.0865e+00,  ..., -6.7507e-01,\n",
      "            1.2710e+00,  8.9607e-01],\n",
      "          [ 5.6680e-01,  7.3008e-01, -8.3506e-01,  ..., -1.0691e+00,\n",
      "            6.7487e-01, -1.4030e+00],\n",
      "          [-8.2623e-01,  2.1436e-01, -1.5980e-01,  ..., -1.0494e+00,\n",
      "            3.8070e-01,  3.7646e-01],\n",
      "          ...,\n",
      "          [-3.0537e-01, -1.0663e+00, -3.5963e-01,  ..., -2.8215e-01,\n",
      "           -1.1096e+00,  1.2409e+00],\n",
      "          [ 5.0949e-01, -3.2463e-01, -2.2986e-01,  ..., -8.6711e-01,\n",
      "           -5.5675e-01,  4.8312e-01],\n",
      "          [-6.7822e-01, -5.4470e-01,  6.5326e-01,  ...,  7.7475e-02,\n",
      "           -1.2378e+00, -7.7889e-01]],\n",
      "\n",
      "         [[-9.4629e-01,  3.0269e-01, -5.5374e-01,  ..., -4.3198e-01,\n",
      "            1.3596e+00,  8.3786e-01],\n",
      "          [-2.9280e-01, -3.2945e-01,  2.3786e-01,  ...,  1.4334e+00,\n",
      "           -1.6649e-01,  1.9197e-01],\n",
      "          [ 1.4988e+00, -4.8714e-02,  6.8606e-01,  ...,  1.6439e-01,\n",
      "            8.8625e-01, -1.8694e-01],\n",
      "          ...,\n",
      "          [-2.5656e+00,  1.0350e+00,  4.6914e-01,  ..., -8.8532e-01,\n",
      "            1.4233e-01,  9.2159e-01],\n",
      "          [ 1.1518e+00, -3.4221e-01, -6.2845e-01,  ..., -6.5063e-01,\n",
      "           -9.0306e-01, -1.5406e-01],\n",
      "          [-1.0904e+00, -2.3287e-01,  1.8758e+00,  ...,  4.1544e-01,\n",
      "            6.5183e-02,  1.7790e-01]],\n",
      "\n",
      "         [[-3.7248e-01, -9.8741e-01, -4.7404e-01,  ...,  6.3597e-01,\n",
      "           -1.5321e-01,  4.1113e-01],\n",
      "          [ 1.1654e+00,  1.8754e+00, -1.7578e-01,  ...,  2.4493e-01,\n",
      "           -4.1139e-01,  8.4194e-01],\n",
      "          [-1.1866e+00, -4.7064e-01, -3.9177e-01,  ..., -1.1570e+00,\n",
      "            3.7199e-01,  3.6645e-02],\n",
      "          ...,\n",
      "          [ 1.8274e+00, -1.3419e+00,  1.2542e+00,  ..., -4.9381e-01,\n",
      "            1.0182e+00, -4.1529e-01],\n",
      "          [ 1.0626e+00, -2.6767e-01,  7.3312e-01,  ...,  8.8181e-02,\n",
      "           -1.7379e+00,  6.5859e-01],\n",
      "          [-2.0477e-01, -2.7443e+00,  1.0011e+00,  ...,  5.5287e-01,\n",
      "           -4.9131e-01,  2.8183e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4486e+00, -8.1718e-01,  1.1845e+00,  ..., -2.5637e-01,\n",
      "            9.4411e-01, -3.9018e-01],\n",
      "          [ 1.1137e-01,  5.1104e-02,  1.1455e-01,  ..., -3.7323e-01,\n",
      "            2.8807e-01,  6.0529e-02],\n",
      "          [-7.2110e-01, -5.3702e-01,  4.9363e-01,  ..., -6.1274e-01,\n",
      "           -1.4482e+00,  5.6125e-01],\n",
      "          ...,\n",
      "          [ 7.8155e-01, -1.3975e+00,  1.0472e+00,  ...,  1.0431e+00,\n",
      "           -4.2385e-01,  6.2266e-01],\n",
      "          [-1.3693e+00, -6.9838e-01, -4.7646e-01,  ...,  2.4893e-01,\n",
      "            2.6184e+00,  7.1876e-01],\n",
      "          [ 1.0881e+00, -4.3688e-01,  1.1055e-01,  ...,  8.1843e-01,\n",
      "           -1.3476e+00,  4.4180e-01]],\n",
      "\n",
      "         [[-1.4313e-01,  4.6824e-01,  5.2465e-01,  ..., -1.1465e+00,\n",
      "            2.6875e-01,  3.9943e-01],\n",
      "          [-1.6079e-01, -1.9586e+00,  2.1287e-01,  ...,  7.0913e-01,\n",
      "           -2.6532e-01, -9.3770e-01],\n",
      "          [ 9.0647e-03, -5.7343e-01,  6.0147e-01,  ...,  1.6351e+00,\n",
      "           -5.5500e-01, -3.0385e+00],\n",
      "          ...,\n",
      "          [-1.3283e-01,  4.6315e-01,  2.1631e-01,  ...,  6.1354e-02,\n",
      "           -2.4660e-01,  7.5512e-01],\n",
      "          [ 3.8715e-02,  1.0491e+00, -1.9710e+00,  ...,  4.1029e-01,\n",
      "           -1.1366e+00,  1.9298e+00],\n",
      "          [ 3.8868e-01,  8.7710e-01,  2.6582e-01,  ..., -2.7272e-01,\n",
      "           -1.4204e+00,  7.9505e-01]],\n",
      "\n",
      "         [[-4.1179e-02, -3.8718e-01,  2.2770e+00,  ...,  1.3394e+00,\n",
      "           -2.9767e-01, -4.2225e-01],\n",
      "          [-4.0886e-02, -3.8847e-01,  2.2796e+00,  ...,  1.3394e+00,\n",
      "           -2.9774e-01, -4.2214e-01],\n",
      "          [-1.2217e+00, -1.6063e-02, -4.6624e-01,  ..., -1.7969e+00,\n",
      "           -4.6751e-01, -2.5298e-01],\n",
      "          ...,\n",
      "          [ 2.2160e-01, -5.1986e-01, -3.0151e-01,  ...,  1.7445e+00,\n",
      "            1.6048e+00, -9.3340e-01],\n",
      "          [-4.0521e-01, -8.5386e-04,  8.9608e-01,  ..., -1.2101e+00,\n",
      "           -1.8775e-01,  3.1494e-01],\n",
      "          [-4.0257e-01, -8.4150e-02,  1.2464e+00,  ...,  1.6827e+00,\n",
      "           -1.4368e+00,  4.6193e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.0910e-01, -7.9057e-01,  6.7217e-01,  ..., -1.2278e+00,\n",
      "            1.9946e+00, -1.8115e+00],\n",
      "          [ 1.1488e-02,  9.1966e-01,  4.4809e-01,  ...,  7.4288e-01,\n",
      "            7.9335e-01, -1.1787e+00],\n",
      "          [-2.9994e-01, -1.3843e+00,  2.6490e-02,  ...,  1.1315e+00,\n",
      "            9.8153e-01,  9.8827e-01],\n",
      "          ...,\n",
      "          [-1.5881e-01, -3.3909e-01,  2.5904e-01,  ...,  1.6118e+00,\n",
      "            7.2214e-01,  3.7117e-01],\n",
      "          [-3.0009e-01, -1.3836e+00,  2.6326e-02,  ...,  1.1315e+00,\n",
      "            9.8104e-01,  9.8827e-01],\n",
      "          [-1.9472e+00,  9.6528e-01, -1.1308e-01,  ..., -2.9268e-02,\n",
      "           -1.3544e+00, -5.1323e-01]],\n",
      "\n",
      "         [[-1.5039e+00,  1.0236e+00, -1.3889e+00,  ..., -6.8819e-01,\n",
      "           -7.9225e-02,  1.2517e+00],\n",
      "          [ 1.6539e+00,  1.9552e+00, -3.0157e-01,  ...,  1.7650e+00,\n",
      "            1.3357e-02, -6.0852e-01],\n",
      "          [ 6.1211e-01,  6.7980e-01, -1.6131e+00,  ..., -1.1116e+00,\n",
      "            1.5719e+00,  2.0253e+00],\n",
      "          ...,\n",
      "          [ 4.8665e-01, -1.3750e-01,  1.0856e-01,  ..., -1.8587e+00,\n",
      "           -8.5056e-02,  1.3738e-01],\n",
      "          [ 1.0429e+00, -1.0416e+00, -4.2302e-01,  ..., -8.7615e-01,\n",
      "           -1.5724e+00,  4.5260e-01],\n",
      "          [ 1.0270e-01, -7.2760e-02,  5.5934e-01,  ...,  7.5342e-01,\n",
      "           -9.7292e-01, -5.7877e-01]],\n",
      "\n",
      "         [[ 1.4648e+00, -2.6031e-02, -1.3449e+00,  ...,  2.2014e-01,\n",
      "           -1.1623e+00, -9.8430e-01],\n",
      "          [-5.2478e-01,  6.7931e-01,  4.5105e-01,  ...,  1.9587e+00,\n",
      "           -9.0774e-02, -5.2714e-01],\n",
      "          [-1.4252e-01, -9.9532e-01,  1.5442e-01,  ...,  2.2336e-01,\n",
      "            1.5048e+00,  9.0822e-01],\n",
      "          ...,\n",
      "          [-1.6586e+00, -1.4976e-01, -1.6148e+00,  ..., -1.0754e+00,\n",
      "           -1.6569e+00, -8.3986e-02],\n",
      "          [ 1.5271e+00,  5.1991e-01, -5.3500e-01,  ...,  1.0007e+00,\n",
      "           -9.7996e-01,  1.0546e+00],\n",
      "          [ 9.5348e-02,  1.2941e+00,  7.6517e-01,  ..., -2.6139e-01,\n",
      "           -6.7663e-01, -8.4169e-01]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "B, H, S, D = 4, 16, 128, 64\n",
    "\n",
    "# Create random input and attention mask\n",
    "inputs = torch.randn(B, S, H * D, device=\"cuda\")\n",
    "attention_mask = torch.randint(0, 2, (B, S), device=\"cuda\").to(torch.bool)\n",
    "# Create packed sequences\n",
    "packed, cu_seqlens = pack_sequences(inputs, attention_mask)\n",
    "\n",
    "query = torch.randn(B, S, H * D, device=\"cuda\")\n",
    "key = torch.randn(B, S, H * D, device=\"cuda\")\n",
    "value = torch.randn(B, S, H * D, device=\"cuda\")\n",
    "\n",
    "query = rearrange(query, \"b s (h d) -> b h s d\", h=H)\n",
    "key = rearrange(key, \"b s (h d) -> b h s d\", h=H)\n",
    "value = rearrange(value, \"b s (h d) -> b h s d\", h=H)\n",
    "\n",
    "# Test naive softpick attention\n",
    "print(\"Testing naive softpick attention...\")\n",
    "naive_attn, _ = naive_softpick_attn(query, key, value, mask=attention_mask, scale=D**0.5, head_first=True)\n",
    "\n",
    "# Test parallel softpick attention\n",
    "print(\"Testing parallel softpick attention...\")\n",
    "packed_query, cu_seqlens = pack_sequences(query, attention_mask)\n",
    "packed_key, _ = pack_sequences(key, attention_mask)\n",
    "packed_value, _ = pack_sequences(value, attention_mask)\n",
    "parallel_attn = parallel_softpick_attn(packed_query, packed_key, packed_value, scale=D**0.5, cu_seqlens=cu_seqlens, head_first=True)\n",
    "\n",
    "print(naive_attn)  # Expected shape: [B, H, S, D]\n",
    "# print(attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The values for attribute 'shape' do not match: torch.Size([4, 16, 128, 64]) != torch.Size([1, 64, 128, 64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnaive_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/testing/_comparison.py:1524\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1502\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1503\u001b[0m     actual,\n\u001b[1;32m   1504\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1519\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1520\u001b[0m )\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1523\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1524\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: The values for attribute 'shape' do not match: torch.Size([4, 16, 128, 64]) != torch.Size([1, 64, 128, 64])."
     ]
    }
   ],
   "source": [
    "torch.testing.assert_close(\n",
    "    naive_attn, parallel_attn, rtol=1e-5, atol=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def run_comparison_test():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device.type == \"cpu\":\n",
    "        return False\n",
    "\n",
    "    dtype = torch.float32\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    B = 1\n",
    "    T_actual = 3\n",
    "    H = 1\n",
    "    K_dim = 2\n",
    "    V_dim = 2\n",
    "    scale = K_dim ** -0.5\n",
    "\n",
    "    q_data = torch.randn(B, T_actual, H, K_dim, device=device, dtype=dtype)\n",
    "    k_data = torch.randn(B, T_actual, H, K_dim, device=device, dtype=dtype)\n",
    "    v_data = torch.randn(B, T_actual, H, V_dim, device=device, dtype=dtype)\n",
    "\n",
    "    q_n_for_naive = q_data.permute(0, 2, 1, 3)\n",
    "    k_n_for_naive = k_data.permute(0, 2, 1, 3)\n",
    "    v_n_for_naive = v_data.permute(0, 2, 1, 3)\n",
    "    naive_bidirectional_mask = torch.ones(T_actual, T_actual, device=device, dtype=torch.bool)\n",
    "\n",
    "    o_naive_heads_first, _ = naive_softpick_attn(\n",
    "        q_n_for_naive, k_n_for_naive, v_n_for_naive, scale,\n",
    "        head_first=True,\n",
    "        mask=naive_bidirectional_mask\n",
    "    )\n",
    "    o_naive = o_naive_heads_first.permute(0, 2, 1, 3)\n",
    "\n",
    "    cu_seqlens_parallel = torch.tensor([0, T_actual], device=device, dtype=torch.long)\n",
    "\n",
    "    o_parallel = parallel_softpick_attn(\n",
    "        q_data, k_data, v_data, scale,\n",
    "        cu_seqlens=cu_seqlens_parallel,\n",
    "        head_first=False\n",
    "    )\n",
    "\n",
    "    return torch.allclose(o_naive, o_parallel, atol=1e-2, rtol=1e-2)\n",
    "\n",
    "# You need to have your `naive_softpick_attn` and `parallel_softpick_attn`\n",
    "# (with the bidirectional modification) defined or imported for this to run.\n",
    "# Then, you can call:\n",
    "result = run_comparison_test()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched outputs are close: True\n"
     ]
    }
   ],
   "source": [
    "def run_batched_comparison_test():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device.type == \"cpu\":\n",
    "        print(\"CUDA not available, skipping batched Triton test.\")\n",
    "        return False\n",
    "\n",
    "    dtype = torch.float32\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    B = 2\n",
    "    T_padded = 5\n",
    "    H = 2\n",
    "    K_dim = 4\n",
    "    V_dim = 4\n",
    "    scale = K_dim ** -0.5\n",
    "\n",
    "    actual_lengths_list = [3, T_padded] # First sequence is shorter\n",
    "    actual_lengths = torch.tensor(actual_lengths_list, device=device, dtype=torch.long)\n",
    "\n",
    "    q_base = torch.randn(B, T_padded, H, K_dim, device=device, dtype=dtype)\n",
    "    k_base = torch.randn(B, T_padded, H, K_dim, device=device, dtype=dtype)\n",
    "    v_base = torch.randn(B, T_padded, H, V_dim, device=device, dtype=dtype)\n",
    "\n",
    "    o_naive_list = []\n",
    "    for i in range(B):\n",
    "        L_i = actual_lengths[i].item()\n",
    "        q_i = q_base[i:i+1, :L_i, ...].permute(0, 2, 1, 3) # [1, H, L_i, K_dim]\n",
    "        k_i = k_base[i:i+1, :L_i, ...].permute(0, 2, 1, 3) # [1, H, L_i, K_dim]\n",
    "        v_i = v_base[i:i+1, :L_i, ...].permute(0, 2, 1, 3) # [1, H, L_i, V_dim]\n",
    "\n",
    "        mask_i = torch.ones(L_i, L_i, device=device, dtype=torch.bool)\n",
    "\n",
    "        o_i_calc, _ = naive_softpick_attn(q_i, k_i, v_i, scale, head_first=True, mask=mask_i)\n",
    "\n",
    "        padded_o_i = torch.zeros(1, H, T_padded, V_dim, device=device, dtype=dtype)\n",
    "        padded_o_i[:, :, :L_i, :] = o_i_calc\n",
    "        o_naive_list.append(padded_o_i)\n",
    "\n",
    "    o_naive_batched_hf = torch.cat(o_naive_list, dim=0) # [B, H, T_padded, V_dim]\n",
    "    o_naive = o_naive_batched_hf.permute(0, 2, 1, 3) # -> [B, T_padded, H, V_dim]\n",
    "\n",
    "    cu_seqlens = torch.cat([torch.tensor([0], device=device, dtype=torch.long),\n",
    "                            torch.cumsum(actual_lengths, dim=0)])\n",
    "\n",
    "    q_packed_list = [q_base[i, :actual_lengths[i].item(), ...] for i in range(B)]\n",
    "    k_packed_list = [k_base[i, :actual_lengths[i].item(), ...] for i in range(B)]\n",
    "    v_packed_list = [v_base[i, :actual_lengths[i].item(), ...] for i in range(B)]\n",
    "\n",
    "    q_par = torch.cat(q_packed_list, dim=0).unsqueeze(0) # [1, total_tokens, H, K_dim]\n",
    "    k_par = torch.cat(k_packed_list, dim=0).unsqueeze(0) # [1, total_tokens, H, K_dim]\n",
    "    v_par = torch.cat(v_packed_list, dim=0).unsqueeze(0) # [1, total_tokens, H, V_dim]\n",
    "\n",
    "    # Call your modified parallel_softpick_attn\n",
    "    # It expects q,k,v as [B,T,H,D] if head_first=False (B=1 for packed)\n",
    "    o_parallel_packed = parallel_softpick_attn(\n",
    "        q_par, k_par, v_par, scale,\n",
    "        cu_seqlens=cu_seqlens,\n",
    "        head_first=False # Inputs are [1, total_T, H, D]\n",
    "    )\n",
    "    # o_parallel_packed is [1, total_tokens, H, V_dim] (if head_first=False output from func)\n",
    "    # If parallel_softpick_attn returns head_first=True output ([1,H,total_tokens,D]), permute it:\n",
    "    # if o_parallel_packed.shape[1] == H: # Heuristic to check if it's head-first\n",
    "    #    o_parallel_packed = o_parallel_packed.permute(0,2,1,3)\n",
    "\n",
    "\n",
    "    # Unpack the output from parallel version\n",
    "    o_parallel_unpacked = torch.zeros_like(o_naive) # Target shape [B, T_padded, H, V_dim]\n",
    "    current_pos = 0\n",
    "    for i in range(B):\n",
    "        L_i = actual_lengths[i].item()\n",
    "        if L_i > 0:\n",
    "            o_parallel_unpacked[i, :L_i, ...] = o_parallel_packed[0, current_pos:current_pos + L_i, ...]\n",
    "        current_pos += L_i\n",
    "\n",
    "    # --- Comparison (only for valid token positions) ---\n",
    "    # Create a mask for positions that are not padding\n",
    "    valid_token_mask_2d = torch.arange(T_padded, device=device).unsqueeze(0) < actual_lengths.unsqueeze(1)\n",
    "    # valid_token_mask_2d is [B, T_padded]\n",
    "\n",
    "    # Expand mask to match output tensor dimensions [B, T_padded, H, V_dim]\n",
    "    valid_token_mask_expanded = valid_token_mask_2d.unsqueeze(-1).unsqueeze(-1).expand_as(o_naive)\n",
    "\n",
    "    # Compare only the valid parts\n",
    "    are_close = torch.allclose(\n",
    "        o_naive[valid_token_mask_expanded],\n",
    "        o_parallel_unpacked[valid_token_mask_expanded],\n",
    "        atol=1e-2, rtol=1e-2 # Tolerances might be needed\n",
    "    )\n",
    "    if not are_close:\n",
    "        # For debugging, print differences only on the valid parts\n",
    "        # Iterate batch items to print manageable chunks\n",
    "        for i in range(B):\n",
    "            L_i = actual_lengths[i].item()\n",
    "            if L_i == 0: continue\n",
    "            # print(f\"Batch item {i}, Len {L_i}\")\n",
    "            # print(\"Naive valid:\", o_naive[i, :L_i].flatten()[:10]) # Print first 10 of flattened valid part\n",
    "            # print(\"Parallel valid:\", o_parallel_unpacked[i, :L_i].flatten()[:10])\n",
    "            # print(\"Diff valid:\", (o_naive[i, :L_i] - o_parallel_unpacked[i, :L_i]).abs().max())\n",
    "            pass # Keep it quiet as per user request\n",
    "    return are_close\n",
    "\n",
    "# To run:\n",
    "# 1. Replace dummy functions with your actual, MODIFIED code.\n",
    "# 2. Then call:\n",
    "result = run_batched_comparison_test()\n",
    "print(f\"Batched outputs are close: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "def pack_qkv_for_parallel_attn(\n",
    "    q_batched: torch.Tensor,\n",
    "    k_batched: torch.Tensor,\n",
    "    v_batched: torch.Tensor,\n",
    "    actual_lengths: torch.LongTensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.LongTensor, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Packs batched Q, K, V tensors for attention mechanisms that use cu_seqlens.\n",
    "\n",
    "    Args:\n",
    "        q_batched (torch.Tensor): Batched queries, shape [B, T_padded, H, K_dim].\n",
    "        k_batched (torch.Tensor): Batched keys, shape [B, T_padded, H, K_dim].\n",
    "        v_batched (torch.Tensor): Batched values, shape [B, T_padded, H, V_dim].\n",
    "        actual_lengths (torch.LongTensor): Tensor of actual sequence lengths, shape [B].\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - q_packed (torch.Tensor): Packed queries, shape [1, total_tokens, H, K_dim].\n",
    "            - k_packed (torch.Tensor): Packed keys, shape [1, total_tokens, H, K_dim].\n",
    "            - v_packed (torch.Tensor): Packed values, shape [1, total_tokens, H, V_dim].\n",
    "            - cu_seqlens (torch.LongTensor): Cumulative sequence lengths, shape [B + 1].\n",
    "            - unpack_info (Dict[str, Any]): Information needed for unpacking.\n",
    "    \"\"\"\n",
    "    original_batch_size = q_batched.shape[0]\n",
    "    original_padded_length = q_batched.shape[1]\n",
    "    num_heads_q = q_batched.shape[2]\n",
    "    head_dim_k = q_batched.shape[3]\n",
    "    num_heads_v = v_batched.shape[2] # Can be different for GQA/MQA for K,V\n",
    "    head_dim_v = v_batched.shape[3]\n",
    "\n",
    "\n",
    "    q_packed_list: List[torch.Tensor] = []\n",
    "    k_packed_list: List[torch.Tensor] = []\n",
    "    v_packed_list: List[torch.Tensor] = []\n",
    "\n",
    "    for i in range(original_batch_size):\n",
    "        len_i = actual_lengths[i].item()\n",
    "        if len_i > 0:\n",
    "            q_packed_list.append(q_batched[i, :len_i, ...])\n",
    "            k_packed_list.append(k_batched[i, :len_i, ...])\n",
    "            v_packed_list.append(v_batched[i, :len_i, ...])\n",
    "\n",
    "    if not q_packed_list: # All sequences had length 0\n",
    "        q_packed = torch.empty(1, 0, num_heads_q, head_dim_k, dtype=q_batched.dtype, device=q_batched.device)\n",
    "        k_packed = torch.empty(1, 0, k_batched.shape[2], k_batched.shape[3], dtype=k_batched.dtype, device=k_batched.device)\n",
    "        v_packed = torch.empty(1, 0, num_heads_v, head_dim_v, dtype=v_batched.dtype, device=v_batched.device)\n",
    "    else:\n",
    "        q_packed = torch.cat(q_packed_list, dim=0).unsqueeze(0)\n",
    "        k_packed = torch.cat(k_packed_list, dim=0).unsqueeze(0)\n",
    "        v_packed = torch.cat(v_packed_list, dim=0).unsqueeze(0)\n",
    "\n",
    "    cu_seqlens = torch.cat([\n",
    "        torch.tensor([0], device=actual_lengths.device, dtype=torch.long),\n",
    "        torch.cumsum(actual_lengths, dim=0)\n",
    "    ])\n",
    "\n",
    "    unpack_info = {\n",
    "        \"original_batch_size\": original_batch_size,\n",
    "        \"original_padded_length\": original_padded_length,\n",
    "        \"actual_lengths\": actual_lengths,\n",
    "        \"q_dtype\": q_batched.dtype,\n",
    "        \"q_device\": q_batched.device\n",
    "    }\n",
    "\n",
    "    return q_packed, k_packed, v_packed, cu_seqlens, unpack_info\n",
    "\n",
    "def unpack_output_from_parallel_attn(\n",
    "    output_packed: torch.Tensor,\n",
    "    unpack_info: Dict[str, Any]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Unpacks the output from an attention mechanism that used packed inputs.\n",
    "\n",
    "    Args:\n",
    "        output_packed (torch.Tensor): Packed output, shape [1, total_tokens, H_out, D_out].\n",
    "        unpack_info (Dict[str, Any]): Information from the packing function.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Unpacked output, shape [B, T_padded, H_out, D_out].\n",
    "    \"\"\"\n",
    "    original_batch_size = unpack_info[\"original_batch_size\"]\n",
    "    original_padded_length = unpack_info[\"original_padded_length\"]\n",
    "    actual_lengths = unpack_info[\"actual_lengths\"]\n",
    "    output_dtype = output_packed.dtype # Use dtype from packed output\n",
    "    output_device = output_packed.device\n",
    "\n",
    "    num_heads_out = output_packed.shape[2]\n",
    "    head_dim_out = output_packed.shape[3]\n",
    "\n",
    "    output_unpacked = torch.zeros(\n",
    "        original_batch_size,\n",
    "        original_padded_length,\n",
    "        num_heads_out,\n",
    "        head_dim_out,\n",
    "        dtype=output_dtype,\n",
    "        device=output_device\n",
    "    )\n",
    "\n",
    "    current_pos = 0\n",
    "    for i in range(original_batch_size):\n",
    "        len_i = actual_lengths[i].item()\n",
    "        if len_i > 0:\n",
    "            output_unpacked[i, :len_i, ...] = output_packed[0, current_pos : current_pos + len_i, ...]\n",
    "        current_pos += len_i\n",
    "\n",
    "    return output_unpacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive output shape: torch.Size([1, 3, 1, 2])\n",
      "Parallel output shape: torch.Size([1, 3, 1, 2])\n",
      "Naive output:\n",
      "tensor([[[[ 0.1183, -0.0333]],\n",
      "\n",
      "         [[ 0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.5512, -0.1587]]]], device='cuda:0')\n",
      "Parallel output:\n",
      "tensor([[[[ 2.5420, -0.7160]],\n",
      "\n",
      "         [[ 0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.5509, -0.1587]]]], device='cuda:0')\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def run_comparison_test():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device.type == \"cpu\":\n",
    "        return False\n",
    "\n",
    "    dtype = torch.float32\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    B = 1\n",
    "    T_actual = 3\n",
    "    H = 1\n",
    "    K_dim = 2\n",
    "    V_dim = 2\n",
    "    scale = K_dim ** -0.5\n",
    "\n",
    "    q_data = torch.randn(B, T_actual, H, K_dim, device=device, dtype=dtype)\n",
    "    k_data = torch.randn(B, T_actual, H, K_dim, device=device, dtype=dtype)\n",
    "    v_data = torch.randn(B, T_actual, H, V_dim, device=device, dtype=dtype)\n",
    "\n",
    "    q_n_for_naive = q_data.permute(0, 2, 1, 3)\n",
    "    k_n_for_naive = k_data.permute(0, 2, 1, 3)\n",
    "    v_n_for_naive = v_data.permute(0, 2, 1, 3)\n",
    "    naive_bidirectional_mask = torch.ones(T_actual, T_actual, device=device, dtype=torch.bool)\n",
    "\n",
    "    o_naive_heads_first, _ = naive_softpick_attn(\n",
    "        q_n_for_naive, k_n_for_naive, v_n_for_naive, scale,\n",
    "        head_first=True,\n",
    "        mask=naive_bidirectional_mask\n",
    "    )\n",
    "    o_naive = o_naive_heads_first.permute(0, 2, 1, 3)\n",
    "\n",
    "    cu_seqlens_parallel = torch.tensor([0, T_actual], device=device, dtype=torch.long)\n",
    "\n",
    "    o_parallel = parallel_softpick_attn(\n",
    "        q_data, k_data, v_data, scale,\n",
    "        cu_seqlens=cu_seqlens_parallel,\n",
    "        head_first=False\n",
    "    )\n",
    "\n",
    "    print(\"Naive output shape:\", o_naive.shape)\n",
    "    print(\"Parallel output shape:\", o_parallel.shape)\n",
    "    print(f\"{'Naive output:'}\\n{o_naive}\")\n",
    "    print(f\"{'Parallel output:'}\\n{o_parallel}\")\n",
    "\n",
    "    return torch.allclose(o_naive, o_parallel, atol=1e-5, rtol=1e-3)\n",
    "\n",
    "# You need to have your `naive_softpick_attn` and `parallel_softpick_attn`\n",
    "# (with the bidirectional modification) defined or imported for this to run.\n",
    "# Then, you can call:\n",
    "result = run_comparison_test()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing PACKED SEQUENCES (cu_seqlens)...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "split_with_sizes expects split_sizes to sum exactly to 2 (input tensor's size at dimension 1), but got split_sizes=[3, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpacked\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39muse_cu_seqlens\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatched\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m test!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[43mtest_softpick_attention_equivalence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 89\u001b[0m, in \u001b[0;36mtest_softpick_attention_equivalence\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cu_seqlens:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# Split concatenated output back into original sequences\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     triton_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     86\u001b[0m         triton_output[:, cu_seqlens[i]:cu_seqlens[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]] \n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq_lens))\n\u001b[1;32m     88\u001b[0m     ]\n\u001b[0;32m---> 89\u001b[0m     naive_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnaive_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     triton_outputs \u001b[38;5;241m=\u001b[39m triton_output\n",
      "File \u001b[0;32m~/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/_tensor.py:917\u001b[0m, in \u001b[0;36mTensor.split\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_VF\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m, split_size, dim)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_with_sizes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: split_with_sizes expects split_sizes to sum exactly to 2 (input tensor's size at dimension 1), but got split_sizes=[3, 4]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.testing import assert_close\n",
    "from einops import rearrange\n",
    "\n",
    "def test_softpick_attention_equivalence():\n",
    "    # Test both packed sequences (cu_seqlens) and batched inputs\n",
    "    for use_cu_seqlens in [True, False]:\n",
    "        print(f\"\\nTesting {'PACKED SEQUENCES (cu_seqlens)' if use_cu_seqlens else 'BATCHED INPUT'}...\")\n",
    "        \n",
    "        # Configuration\n",
    "        num_sequences = 2  # Actual batch size for non-packed case\n",
    "        max_seq_len = 4\n",
    "        num_heads = 2\n",
    "        head_dim = 8\n",
    "        hidden_size = num_heads * head_dim\n",
    "        scale = 1.0 / (head_dim ** 0.5)\n",
    "        dtype = torch.float32\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Create different sequence lengths\n",
    "        seq_lens = [3, 4] if use_cu_seqlens else [max_seq_len]*num_sequences\n",
    "        \n",
    "        # Generate test data --------------------------------------------------\n",
    "        if use_cu_seqlens:\n",
    "            # Packed format: single batch with concatenated sequences\n",
    "            q_seqs = [torch.randn(1, l, hidden_size, device=\"cuda\") for l in seq_lens]\n",
    "            k_seqs = [torch.randn(1, l, hidden_size, device=\"cuda\") for l in seq_lens]\n",
    "            v_seqs = [torch.randn(1, l, hidden_size, device=\"cuda\") for l in seq_lens]\n",
    "            \n",
    "            q = torch.cat(q_seqs, dim=1)\n",
    "            k = torch.cat(k_seqs, dim=1)\n",
    "            v = torch.cat(v_seqs, dim=1)\n",
    "            \n",
    "            # Create cu_seqlens [0, 3, 7] for seq_lens [3,4]\n",
    "            cu_seqlens = torch.tensor(\n",
    "                [0] + [sum(seq_lens[:i+1]) for i in range(len(seq_lens))],\n",
    "                dtype=torch.long, device=\"cuda\"\n",
    "            )\n",
    "            attention_mask = None  # Masking handled via cu_seqlens\n",
    "        else:\n",
    "            # Batched format: separate entries in batch dimension\n",
    "            q = torch.randn(num_sequences, max_seq_len, hidden_size, device=\"cuda\")\n",
    "            k = torch.randn(num_sequences, max_seq_len, hidden_size, device=\"cuda\")\n",
    "            v = torch.randn(num_sequences, max_seq_len, hidden_size, device=\"cuda\")\n",
    "            \n",
    "            # Create attention mask (1=valid, 0=pad)\n",
    "            attention_mask = torch.zeros(num_sequences, max_seq_len, device=\"cuda\")\n",
    "            for i,l in enumerate(seq_lens):\n",
    "                attention_mask[i, :l] = 1\n",
    "            cu_seqlens = None\n",
    "\n",
    "        # Convert to head-first format -----------------------------------------\n",
    "        q_head_first = rearrange(q, \"b s (h d) -> b h s d\", h=num_heads)\n",
    "        k_head_first = rearrange(k, \"b s (h d) -> b h s d\", h=num_heads)\n",
    "        v_head_first = rearrange(v, \"b s (h d) -> b h s d\", h=num_heads)\n",
    "\n",
    "        # Run implementations -------------------------------------------------\n",
    "        # Triton implementation\n",
    "        triton_output = parallel_softpick_attn(\n",
    "            q_head_first,\n",
    "            k_head_first,\n",
    "            v_head_first,\n",
    "            scale=scale,\n",
    "            cu_seqlens=cu_seqlens if use_cu_seqlens else None,\n",
    "            head_first=True\n",
    "        )\n",
    "        \n",
    "        # Naive implementation\n",
    "        naive_output, _ = naive_softpick_attn(\n",
    "            q_head_first,\n",
    "            k_head_first,\n",
    "            v_head_first,\n",
    "            scale=scale,\n",
    "            cu_seqlens=cu_seqlens if use_cu_seqlens else None,\n",
    "            head_first=True,\n",
    "            mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # Convert outputs ------------------------------------------------------\n",
    "        triton_output = rearrange(triton_output, \"b h s d -> b s (h d)\")\n",
    "        \n",
    "        # For packed sequences: unpack results\n",
    "        if use_cu_seqlens:\n",
    "            # Split concatenated output back into original sequences\n",
    "            triton_outputs = [\n",
    "                triton_output[:, cu_seqlens[i]:cu_seqlens[i+1]] \n",
    "                for i in range(len(seq_lens))\n",
    "            ]\n",
    "            naive_outputs = naive_output.split(seq_lens, dim=1)\n",
    "        else:\n",
    "            triton_outputs = triton_output\n",
    "            naive_outputs = naive_output\n",
    "\n",
    "        # Validate results ----------------------------------------------------\n",
    "        for i, (triton_out, naive_out) in enumerate(zip(triton_outputs, naive_outputs)):\n",
    "            print(f\"\\nSequence {i+1} comparison:\")\n",
    "            print(\"Triton:\", triton_out[0,:seq_lens[i]].flatten()[:5])\n",
    "            print(\"Naive: \", naive_out[0,:seq_lens[i]].flatten()[:5])\n",
    "            \n",
    "            assert_close(\n",
    "                triton_out[:, :seq_lens[i]],  # Handle padding in batched case\n",
    "                naive_out[:, :seq_lens[i]],\n",
    "                rtol=1e-3,\n",
    "                atol=1e-5,\n",
    "                check_dtype=False,\n",
    "                msg=f\"Mismatch in {'packed' if use_cu_seqlens else 'batched'} seq {i+1}\"\n",
    "            )\n",
    "\n",
    "        print(f\"Passed {'packed' if use_cu_seqlens else 'batched'} test!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_softpick_attention_equivalence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
